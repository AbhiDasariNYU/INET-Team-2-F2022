# -*- coding: utf-8 -*-
"""Copy of (SS)SerenitySearch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tWnaTpx26qChgjxpUpQ4Cs4yIQBlXnXn

# Library
"""

pip install openclean-core

pip install geopy #use to convert coordinates to zipcode

# Commented out IPython magic to ensure Python compatibility.
import os
git_folder = 'INET-Team-2-F2022'
if not os.path.isdir(git_folder):
  !git clone https://github.com/gcivil-nyu-org/INET-Team-2-F2022
else:
#   %cd /tree/develop/Data
  !git pull
#   %cd ..

from openclean.pipeline import stream
from openclean.profiling.column import DefaultColumnProfiler

from openclean.cluster.knn import knn_clusters, knn_collision_clusters
from openclean.function.similarity.base import SimilarityConstraint
from openclean.function.similarity.text import LevenshteinDistance
from openclean.function.token.ngram import NGrams
from openclean.function.value.threshold import GreaterThan
from openclean.operator.transform.update import update
import pandas as pd



"""# 2018 Central Park Squirrel Data"""

url ="https://raw.githubusercontent.com/gcivil-nyu-org/INET-Team-2-F2022/develop/Data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv"
dataset1 = pd.read_csv(url)
ds1 = stream(dataset1)

print('Schema\n------')
for col in ds1.columns:
    print("  '{}'".format(col))
    
print('\n{} rows.'.format(ds1.count()))
print("There are {} rows and {} columns in the dataset.".format(ds1.count(),len(ds1.columns)))

ds1.head()

# https://github.com/gcivil-nyu-org/INET-Team-2-F2022/blob/develop/Data/DOF_Condominium_Comparable_Rental_Income_in_NYC.csv

# ds2.head()

"""#DOF Condominium Comparable Rental Income"""

ur2 ="https://raw.githubusercontent.com/gcivil-nyu-org/INET-Team-2-F2022/develop/Data/DOF_Condominium_Comparable_Rental_Income_in_NYC.csv"
dataset2 = pd.read_csv(ur2)
ds2 = stream(dataset2)

print('Schema\n------')
for col in ds2.columns:
    print("  '{}'".format(col))
    
print('\n{} rows.'.format(ds2.count()))
print("There are {} rows and {} columns in the dataset.".format(ds2.count(),len(ds2.columns)))

ds2.head()

"""# NYPD Arrest Data """

ur3 ="https://raw.githubusercontent.com/gcivil-nyu-org/INET-Team-2-F2022/develop/Data/NYPD_Arrest_Data__Year_to_Date_.csv"
dataset3 = pd.read_csv(ur3)
ds3 = stream(dataset3)

print('Schema\n------')
for col in ds3.columns:
    print("  '{}'".format(col))
    
print('\n{} rows.'.format(ds3.count()))
print("There are {} rows and {} columns in the dataset.".format(ds3.count(),len(ds3.columns)))

ds3.head()

ds3_sample = dataset3[0:10]
ds3_sample

import geopy
geo_locator = geopy.Nominatim(user_agent='1234')

def coord_to_zipcode(point): # (lati, longti)
    r = geo_locator.reverse(point)
    return r.raw['address']['postcode']

result = ds3_sample.apply(lambda row: coord_to_zipcode((row[16],row[17])), axis=1)
result

"""# Parks Properties"""

ur4 ="https://raw.githubusercontent.com/gcivil-nyu-org/INET-Team-2-F2022/develop/Data/Parks_Properties.csv"
dataset4 = pd.read_csv(ur4)
ds4 = stream(dataset4)

print('Schema\n------')
for col in ds4.columns:
    print("  '{}'".format(col))
    
print('\n{} rows.'.format(ds4.count()))
print("There are {} rows and {} columns in the dataset.".format(ds4.count(),len(ds4.columns)))

ds4.head()

"""# NYC Community Based Organizations"""

ur5 ="https://raw.githubusercontent.com/gcivil-nyu-org/INET-Team-2-F2022/develop/Data/Parks_Properties.csv"
dataset5 = pd.read_csv(ur5)
ds5 = stream(dataset5)

print('Schema\n------')
for col in ds5.columns:
    print("  '{}'".format(col))
    
print('\n{} rows.'.format(ds5.count()))
print("There are {} rows and {} columns in the dataset.".format(ds5.count(),len(ds5.columns)))

ds5.head()



"""DOB Permit Issuance"""

ur6 ="https://raw.githubusercontent.com/gcivil-nyu-org/INET-Team-2-F2022/develop/Data/DOB%20Permit%20Issuance.csv"
dataset6 = pd.read_csv(ur6)
ds6 = stream(dataset6)

print('Schema\n------')
for col in ds6.columns:
    print("  '{}'".format(col))
    
print('\n{} rows.'.format(ds6.count()))
print("There are {} rows and {} columns in the dataset.".format(ds6.count(),len(ds6.columns)))

dataset6.drop(dataset6.columns.difference(['job_type','zip_code']), 1, inplace=True)

print('Schema\n------')
for col in dataset6.columns:
    print("  '{}'".format(col))
    

dataset6["job_impact_score"] = " "

# Expected values are:
# • A1 = Alteration Type I, A major alteration that will change the use, egress, or occupancy of the building.
# • A2 = Alteration Type II, An application with multiple types of work that do not affect the use, egress, or occupancy of the building.
# • A3 = Alteration Type III, One type of minor work that doesn't affect the use, egress, or occupancy of the building.
# • NB = New Building, An application to build a new structure. “NB” cannot be selected if any existing building elements are to remain—for example a part of
# an old foundation, a portion of a façade that will be incorporated into the construction, etc.
# • DM = Demolition, An application to fully or partially demolish an existing building.
# • SG = Sign, An application to install or remove an outdoor sign.

#Mapping (Essentially scores on the impact the work will have, higher score means more impact):
# A1 = 4
# A2 = 3
# A3 = 2
# NB = 10
# DM = 10
# SG = 1

for index, row in dataset6.iterrows():
  if row["job_type"] == "A1":
    row["job_impact_score"] = 4
  elif row["job_type"] == "A2":
    row["job_impact_score"] = 3
  elif row["job_type"] == "A3":
    row["job_impact_score"] = 2
  elif row["job_type"] == "NB":
    row["job_impact_score"] = 10
  elif row["job_type"] == "DM":
    row["job_impact_score"] = 10
  elif row["job_type"] == "SG":
    row["job_impact_score"] = 1
  else:
    row["job_impact_score"] = 0
  # jobtype = row["job_type"]
  # impact = row["job_impact_score"]
  # print(f"{jobtype} {impact}")
  


print('\n{} rows.'.format(dataset6.count()))
print("There are {} rows and {} columns in the dataset.".format(dataset6.count(),len(dataset6.columns)))

